---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Shilpi Karan sk46966

### Introduction 

The dataset I chose for this project is Budget Share of Food for Spanish Households. There are 6 variables in the dataset. wfood is the percentage of total expenditure which the household has spent on food, totexp is the total expenditure of the household, age is the age of reference person in the household, size is the size of the household, town is the size of the town where the household is placed categorized into 5 groups: 1 for small towns, 5 for big ones, and sex is the sex of reference person, man or woman. I found the data from a website that has a list of R datasets from a few common packages called https://vincentarelbundock.github.io/Rdatasets/datasets.html. The variables are measuring different aspects of a household that affect food related expenses. There are 23972 observations. There are 20624 observations for the man group and 3347 observations for the woman group for the binary variable, sex.

```{R}
library(tidyverse)
library(dplyr)
library(ggplot2)

data <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/BudgetFood.csv")
data
table(data$sex)
```

### Cluster Analysis

```{R}
library(cluster)

Data <- data %>% select(-1) 
Data <- Data %>% select(-6)
Data

sil_width <- vector()
for(i in 2:10){
  kms <- kmeans(Data, centers=i)
  sil <- silhouette(kms$cluster,dist(Data))
  sil_width[i] <- mean(sil[,3])
}

ggplot() + geom_line(aes(x=1:10,y=sil_width)) + scale_x_continuous(name = "k", breaks=1:10)
data_pam <- data %>% pam(k=2)
data_pam$silinfo$avg.width
data_pam


#Clustering with three measurements

clust_dat<-data%>%dplyr::select(wfood,totexp,age)
set.seed(322)
pam1<-clust_dat%>%pam(k=2)
pam1

pamclust<-clust_dat%>%mutate(cluster=as.factor(pam1$clustering)) #save the cluster solution in your dataset
pamclust%>%ggplot(aes(wfood,totexp,age,sex, color=cluster))+geom_point() #visualize it

pamclust%>%group_by(cluster)%>%summarize_if(is.numeric,mean,na.rm=T)

data%>%slice(pam1$id.med)

#Clustering with all measurements

final <-data %>% select(-X1) %>% select(-sex) %>% scale %>% as.data.frame
pam2 <- final %>% pam(2)

#now that we ran PAM, save the cluster assignment in the dataset
final <- final %>% mutate(cluster=as.factor(pam2$clustering))


library(GGally)
ggpairs(final, aes(color=cluster))

```

It seemed appropriate to only have 2 clusters for the data based on the silhouette width. There seems to be 2 distinct clusters. The goodness of fit is in the center of the clusters which shows that it is a decent goodness of fit.
    
    
### Dimensionality Reduction with PCA

```{R}
data1<- data %>% select(-X1, -sex)
data1
data1_nums <- data1 %>% select_if(is.numeric) %>% scale
rownames(data1_nums)<-data1$Name
data1_pca<-princomp(data1_nums)
names(data1_pca)

summary(data1_pca, loadings=T)

eigval<-data1_pca$sdev^2
varprop=round(eigval/sum(eigval), 2)
ggplot() + geom_bar(aes(y=varprop, x=1:5), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:5)) + geom_text(aes(x=1:5, varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + scale_x_continuous(breaks = 1:10)

round(cumsum(eigval)/sum(eigval), 2)
eigval

summary(data1_pca, loadings=T)

eig1 <- data1 %>% cor %>% eigen
eig1

library(factoextra)
fviz_pca_biplot(data1_pca)
```

Only some of the total variance is explained by these PCs. A high score on each PC retained means that the data is varies a lot. In relation to this dataset, since there is a high pc score, it means that the expenses varies a lot in terms of every other variable in the dataset.

###  Linear Classifier

```{R}
# linear classifier code here

data$sex<-ifelse(data$sex=="man",1,0)
data
library(pROC)
library(plotROC)
ROCplot<-ggplot()+geom_roc(aes(d=data$sex,m=data$wfood + data$totexp + data$age + data$size + data$town), n.cuts=0) 
ROCplot
calc_auc(ROCplot)

data %>% ggplot(aes(wfood + totexp + age + size + town, sex))+geom_point()+geom_smooth(method="lm", se=F)+ylim(0,1)
fit <- lm(sex ~ wfood + totexp + age + size + town, data=data)
score <- predict(fit)
score %>% round(3)
data %>% mutate(score=1) %>% ggplot(aes(wfood + totexp + age + size + town, sex)) + geom_point(aes(color=score>.5)) + geom_smooth(method="lm", se=F)+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff, levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
}
#summarise_all(diags,mean)

```

```{R}
# cross-validation of linear classifier here
data$sex<-ifelse(data$sex=="man",1,0)
data$X1<-NULL
data
fit <- glm(sex~wfood + totexp + age + size + town,data=data,family="binomial") #fit model
prob <- predict(fit,type="response")
prob

class_diag(prob,data$sex,positive=1)

set.seed(1234)
k=10 #choose number of folds
Data<-data[sample(nrow(data)),] #randomly order rows
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-Data[folds!=i,] 
  test<-Data[folds==i,]
  truth<-test$sex ## Truth labels for fold i
  ## Train model on training set (all but fold i)
  fit<-glm(sex~wfood + totexp + age + size + town,data=train,family="binomial")
  ## Test model on test set (fold i) 
  probs<-predict(fit,newdata = test,type="response")
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff, levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
}
#summarize_all(diags,mean) #average diagnostics across all k folds

```

The model is fitting fair per the AUC from the regression. In the cross-validation however, the AUC increased to .76. This is a sign that shows there was not overfitting occurring in the data/model.

### Non-Parametric Classifier

```{R}
library(caret)
library(dplyr)
library(rpart); library(rpart.plot)
fit<- rpart(sex~wfood+totexp+age+size+town, data=data)
rpart.plot(fit)

fit<- rpart(sex~totexp+age+size+town, data=data)
rpart.plot(fit)

fit<- rpart(sex~age+size+town, data=data)
rpart.plot(fit)

fit<- rpart(sex~size + town, data=data)
rpart.plot(fit)

fit<- rpart(sex~town, data=data)
rpart.plot(fit)

#rpart.plot(fit$finalModel)

```

```{R}
# cross-validation of np classifier here
#data$sex<-ifelse(data$sex=="man",1,0)
na.exclude(data)
set.seed(1234)
cv <- trainControl(method="cv", number = 10, classProbs = T, savePredictions = T)
#fit <- train(sex~wfood+totexp+age+size+town, data=data, trControl=cv, method="rpart")
#class_diag(fit$pred$man, fit$pred$woman, positive="man")

```

The new model is overfitting per CV AUC. Both the nonparametric and linear models are similar in its cross-validation performance.


### Regression/Numeric Prediction

```{R}
fit<-lm(wfood~.,data=data)
yhat<-predict(fit)
cbind(yhat, y=data$wfood)

mean((data$wfood-yhat)^2)
```

```{R}
set.seed(1234)
k=5
data<-data[sample(nrow(data)),]
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  fit<-lm(wfood~.,data=train)
  yhat<-predict(fit,newdata=test)
  diags<-mean((test$wfood-yhat)^2) 
}
mean(diags)
```

This model does not show signs of overfititng. The MSE is lower in the cross validation which means there is not overfitting. The first part of this section involved fitting a linear regression model to the entire dataset, predicting food expenses from all other variables.

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3")

plot <- import("matplotlib")
plot$use("Agg", force = TRUE)

#py$data1

s<-"Shilpi"
#cat(c(s,py$s))
```

```{python}
#data1=r.data
#data1
#print(r.data.head())

k="Karan"
print(r.s,k)
```

A Python code chunk was added to show that both python and R work similarly and reticulate was used to demonstrate that you can share objects between R and python using `r.` and `py$`. Since in python, it is stated that data1=r.data, when printing (r.data1.head()), it shows the first few variables and data information from the dataset from r. Another example unrelated to the dataset was used since it was not specified if the code needs to pretain to the dataset. Inn this example, words were exchanged between R and python to combine together and form a name.



