---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Shilpi Karan sk46966

### Introduction 

The dataset I chose for this project is Budget Share of Food for Spanish Households. There are 6 variables in the dataset. wfood is the percentage of total expenditure which the household has spent on food, totexp is the total expenditure of the household, age is the age of reference person in the household, size is the size of the household, town is the size of the town where the household is placed categorized into 5 groups: 1 for small towns, 5 for big ones, and sex is the sex of reference person, man or woman. I found the data from a website that has a list of R datasets from a few common packages called https://vincentarelbundock.github.io/Rdatasets/datasets.html. The variables are measuring different aspects of a household that affect food related expenses. There are 23972 observations. There are 20624 observations for the man group and 3347 observations for the woman group for the binary variable, sex.

```{R}
library(tidyverse)

data <- read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/BudgetFood.csv")
data
table(data$sex)
```

### Cluster Analysis

```{R}
library(cluster)
Data <- data %>% select(-7)
Data <- Data %>% select(-1)
sil_width<-vector()
for(i in 2:10){  
  kms <- kmeans(Data, center=i)
  sil <- silhouette(kms$cluster,dist(Data))
  sil_width[i] <- mean(sil[,3])
}
ggplot() + geom_line(aes(x=1:10, y=sil_width)) + scale_x_continuous(name = "k", breaks=1:10)
data_pam <- Data%>% pam(k=2)
data_pam$silinfo$avg.width
data_pam
  #pam_fit <- pam(pam_dat, k = i)  
  #sil_width[i] <- pam_fit$silinfo$avg.width  
#}
#pam1 <- clust_dat%>%pam(k=2)
#pam1
clust_dat <- Data%>%dplyr::select(wfood,totexp,age)
pamclust<-clust_dat %>% mutate(cluster=as.factor(pam1$clustering))
pamclust %>% ggplot(aes(wfood,totexp,age,color=cluster)) + geom_point()
pamclust %>% group_by(cluster) %>% summarize_if(is.numeric,mean,na.rm=T)
Data%>%slice(pam1$id.med)
pam1$silinfo$avg.width
plot(pam1,which=2)
pam_dat<-Data%>%select(wfood,totexp,age)

ggplot()+geom_line(aes(x=1:3,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)
plot(pam1,which=2)
library(GGally)
ggpairs(data, columns=1:3, aes(color=cluster))
```

It seemed appropriate to only have 2 clusters for the data based on the silhouette width. There seems to be 2 distinct clusters. The goodness of fit is in the center of the cluster which shows that it is a decent goodness of fit.
    
    
### Dimensionality Reduction with PCA

```{R}
data1<- data %>% select(-X1, -sex)
data1
data1_nums <- data %>% select_if(is.numeric) %>% scale
rownames(data1_nums)<-data1$Name
data1_pca<-princomp(data1_nums)
names(data1_pca)

summary(data1_pca, loadings=T)

eigval<-data1_pca$sdev^2
varprop=round(eigval/sum(eigval), 2)
ggplot() + geom_bar(aes(y=varprop, x=1:6), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:6)) + geom_text(aes(x=1:6, varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) +
scale_x_continuous(breaks = 1:10)

round(cumsum(eigval)/sum(eigval), 2)
eigval

summary(data1_pca, loadings=T)

eig1 <- data1 %>% cor %>% eigen
eig1

library(factoextra)
fviz_pca_biplot(data1_pca)
```

Only some of the total variance is explained by these PCs. A high score on each PC retained means that the data is varies a lot. In relation to this dataset, since there is a high pc score, it means that the expenses varies a lot in terms of every other variable in the dataset.

###  Linear Classifier

```{R}
# linear classifier code here

#data$sex<-ifelse(data$sex=="man",1,0)
data
library(pROC)
library(plotROC)
ROCplot<-ggplot()+geom_roc(aes(d=data$sex,m=data$wfood + data$totexp + data$age + data$size + data$town), n.cuts=0) 
ROCplot
calc_auc(ROCplot)

data%>% ggplot(aes(wfood + totexp + age + size + town, sex))+geom_point()+geom_smooth(method="lm", se=F)+ylim(0,1)
fit <- lm(sex ~ wfood + totexp + age + size + town, data=data)
score <- predict(fit)
score %>% round(3)
data %>% mutate(score=1) %>% ggplot(aes(wfood + totexp + age + size + town,sex))+geom_point(aes(color=score>.5))+
  geom_smooth(method="lm", se=F)+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

class_diag(score,truth=data$sex, positive=1)

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
}
```

```{R}
# cross-validation of linear classifier here
fit<- glm(data$sex~wfood,data=data,family="binomial")
prob<-predict(fit,type="response")
prob
class_diag(prob,data$sex,positive=1)
diags1<-class_diag(probs, one$sex, positive = 1)

one<-data %>% sample_frac(.5)
two<-data %>% anti_join(one)

fit<-glm(sex~wfood,data=two,family="binomial")
probs2<-predict(fit,newdata = one,type="response")
diags2<-class_diag(probs2, one$sex, positive = 1)
diags<-rbind(diags1,diags2)
diags
summarize_all(diags,mean)

set.seed(1234)
k=10
data<-data[sample(nrow(data)),]
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$sex
  ## Train model on training set
  fit<-glm(sex~wfood,data=train,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}
```

The model is fitting fair per the AUC from the regression. In the cross-validation however, the AUC dropped. This is a sign of overfitting occurring in the data/model. It was fair before and afterwards it was

### Non-Parametric Classifier

```{R}
library(caret)
#data$sex<-ifelse(data$sex=="man",1,0)

data %>% select(1:6) %>% dist() %>% as.matrix %>% as.data.frame %>% rownames_to_column %>% pivot_longer(-1) %>% left_join(select(rownames_to_column(data),rowname,sex), by="rowname") %>% group_by(name) %>% slice_min(value, n=5) %>% summarize(y_hat=mean(sex)) 

library(caret)
knn_fit <- knn3(factor(sex==1,levels=c("TRUE","FALSE")) ~ wfood, totexp, age, size, town, data=data, k=5)

y_hat_knn <- predict(knn_fit,data)

y_hat_knn

data.frame(y_hat_knn,names=rownames(data)) %>% arrange(names) #The names are distinguished as numbers. 

table(truth= factor(data$sex==1, levels=c("TRUE","FALSE")),
prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))

class_diag(y_hat_knn[,1],data$sex, positive=1)

```

```{R}
# cross-validation of np classifier here
set.seed(1234)
k=10 #choose number of folds

data<-data[sample(nrow(data)),] #randomly order rows
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F) #create 10 folds

diags<-NULL
for(i in 1:k){
  
  ## Create training and test sets
  train<-data33[folds!=i,]
  test<-data33[folds==i,]
  truth<-test$y
  
  ## Train model on training set
  fit<-knn3(y~.,data=train)
  probs<-predict(fit,newdata = test)[,2]
  
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

summarize_all(diags,mean)
```

The new model is overfitting per CV AUC. Both the nonparametric and linear models are similar in its cross-validation performance.


### Regression/Numeric Prediction

```{R}
fit<-lm(wfood~.,data=data)
yhat<-predict(fit)
cbind(yhat, y=data$wfood)

mean((data$wfood-yhat)^2)
```

```{R}
set.seed(1234)
k=5
data<-data[sample(nrow(data)),]
folds<-cut(seq(1:nrow(data)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  fit<-lm(wfood~.,data=train)
  yhat<-predict(fit,newdata=test)
  diags<-mean((test$wfood-yhat)^2) 
}
mean(diags)
```

This model does not show signs of overfititng. The MSE is lower in the cross validation which means there is not overfitting. The first part of this section involved fitting a linear regression model to the entire dataset, predicting food expenses from all other variables.

### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3")
plot <- import("matplotlib")
plot$use("Agg", force = TRUE)

py$data1
```

```{python}
data1=r.data
data1
print(r.data.head())
```

A Python code chunk was added to show that both python and R work similarly and reticulate was used to demonstrate that you can share objects between R and python using `r.` and `py$`. Since in python, it is stated that data1=r.data, when printing (r.data1.head()), it shows the first few variables and data information from the dataset from r.



